{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Seizures and Epilepsy Milestones 4 & 5\n",
    "\n",
    "## Harvard CS 109A\n",
    "\n",
    "### November 28, 2016\n",
    "\n",
    "###### Sean Keery & Shivas Jayaram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring various factors that might affect epilepsy and seizure recurrence\n",
    "\n",
    "Well since milestone 3, we've obtained more data.  The  EEG and drug study data we used for that milestone was very limited in it's predictive ability for general populations.  The additional data was from the UK data service.  It consisted of the 1958 birth sweep and three more sweeps over the next twenty-one years.  It included almost 1800 variables over 19000 unique cases.  We had gone to the other end of the spectrum.\n",
    "\n",
    "However, after inspecting the data, we determined that values for many of the variables were not present.  Due to time constraints, we used means and medians to fill in the absent values.  Input from two ancillary surveys were merged with the original file to enhance demographic components.\n",
    "\n",
    "After our initial review, we identified some variables which allowed us to categorize subjects with epilepsy or a history of seizures.  Fitting our models based on these categorizations helped us to identify variables with predictive power.  Unfortunately, we had missed some variables which should have been used in our first classification.  So we excluded these from our model too.  After re-fitting, there are still some questions in our minds whether we are able to effectively identify factors in recurrence.  We believe that some of the factors identified through Principal Component Analysis and sklearn feature selection aren't necessarily causes of recurrence.  We will continue to explore this moving forward.\n",
    "\n",
    "# Milestone 4 - Baseline Models\n",
    "\n",
    "#### Data Set 1 Thall & Vail\n",
    "\n",
    "This dataset gives a two-week seizure counts for 59 epileptics. The number of seizures was recorded for a baseline period of 8 weeks, and then patients were randomly assigned to a treatment group or a control group. Counts were then recorded for four successive two-week periods. The subject's age is the only covariate.\n",
    "\n",
    "#### Data Set 2 NCDS Sweeps 0 to 3\n",
    "\n",
    "The National Child Development Study (NCDS) originated in the Perinatal Mortality Survey, which examined social and obstetric factors associated with still birth and infant mortality among over 17,000 babies born in Britain in one week in March 1958.  The study has broadened in scope to chart many aspects of the health, educational, and social development of cohort members as they passed through childhood and adolescence. We used results from the first three sweeps (1965 at age 7, 1969 at age 11, 1974 at age 16) in addition to the birth data.\n",
    "\n",
    "Surviving members of this birth cohort have been surveyed on five more occasions in order to monitor their changing health, education, social and economic circumstances.  We have excluded these datasets at this time in order to establish our baselines models.\n",
    "\n",
    "##### Models\n",
    "\n",
    "We have decided that the performance metric to evaluate prediction will be seizure recurrence.\n",
    "\n",
    "Features extracted using PCA and SelectKBest include: \n",
    "- Features extracted from the entire feature set: n1827, n604, n39, n1263, n1400, n1399, n1453, n1476, n825, n2598, n1896, n1898, dvht07, OUTCME01, OUTCME02\n",
    "- Features extracted from female patients: n1region, n514, n383, n1819, n1827, n1828, n1400, n1399, n825, n1896, n1898, dvht07, dvrwt07, dvwt07, OUTCME01\n",
    "- Features extracted from male patients: n400, n403, n1827, n604, n39, n92, n1400, n1399, n1476, n1548, n1551, n825, n1896, OUTCME01, OUTCME02\n",
    "\n",
    "\n",
    "Linear regression provided us with about sixty percent accuracy using the features we had selected above.\n",
    "\n",
    "LDA, QDA, Trees and others also provided the same accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Explore and Clean Data\n",
    "\n",
    "First task will be to read, explore, and clean the data. Our first version of the data exploration can be found [here](ExploreData.ipynb). There was a lot of exploratory and debugging code, so we branched off ot this notebook with the findings we thought would be appropriate for this milestone of the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "from sklearn import preprocessing\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "from sklearn.tree import DecisionTreeClassifier as DecisionTree\n",
    "from sklearn.ensemble import RandomForestClassifier as RandomForest\n",
    "from sklearn.ensemble import AdaBoostClassifier as AdaBoost\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn import grid_search\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import feature_selection as fs\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data: (18558, 1765)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ncdsid</th>\n",
       "      <th>n622</th>\n",
       "      <th>n0region</th>\n",
       "      <th>n1region</th>\n",
       "      <th>n2region</th>\n",
       "      <th>n3region</th>\n",
       "      <th>n553</th>\n",
       "      <th>n545</th>\n",
       "      <th>n520</th>\n",
       "      <th>n490</th>\n",
       "      <th>...</th>\n",
       "      <th>n1849</th>\n",
       "      <th>dvht07</th>\n",
       "      <th>dvht11</th>\n",
       "      <th>dvht16</th>\n",
       "      <th>dvrwt07</th>\n",
       "      <th>dvrwt11</th>\n",
       "      <th>dvrwt16</th>\n",
       "      <th>dvwt07</th>\n",
       "      <th>dvwt11</th>\n",
       "      <th>dvwt16</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>N10001N</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>23</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>1.21899986267032</td>\n",
       "      <td>1.47299957275415</td>\n",
       "      <td>1.59999942779607</td>\n",
       "      <td>110.347991943347</td>\n",
       "      <td>98.1929931640604</td>\n",
       "      <td>105.055999755876</td>\n",
       "      <td>25.8549957275385</td>\n",
       "      <td>37.6489868164152</td>\n",
       "      <td>56.0199890136717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>N10002P</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>34</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>1.34599971771224</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>90.865997314449</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>26.3089904785155</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>N10003Q</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>34</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>1.32099914550831</td>\n",
       "      <td>1.49899959564243</td>\n",
       "      <td>1.87999916076665</td>\n",
       "      <td>87.9599914550983</td>\n",
       "      <td>96.4049987792867</td>\n",
       "      <td>89.382995605487</td>\n",
       "      <td>24.4939880371087</td>\n",
       "      <td>38.1019897460905</td>\n",
       "      <td>66.6799926757659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>N10004R</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>1.29499912262003</td>\n",
       "      <td>1.51099967956562</td>\n",
       "      <td>1.62999916076665</td>\n",
       "      <td>105.16198730471</td>\n",
       "      <td>111.588989257796</td>\n",
       "      <td>132.054992675766</td>\n",
       "      <td>28.122985839843</td>\n",
       "      <td>45.8139953613169</td>\n",
       "      <td>72.7999877929584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>N10005S</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>25</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>1.34599971771224</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>129.382995605487</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>37.6489868164152</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1765 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    ncdsid n622 n0region n1region n2region n3region n553 n545 n520 n490  \\\n",
       "0  N10001N    2        9        9        9        9   23    4    2   12   \n",
       "1  N10002P    1        9        8        8        8   34    4    5    1   \n",
       "2  N10003Q    1        4        4        4        4   34    4   10    1   \n",
       "3  N10004R    2        1        1        1        1   26    4   11    1   \n",
       "4  N10005S    2       10       10       10       10   25    4    1    3   \n",
       "\n",
       "         ...        n1849            dvht07            dvht11  \\\n",
       "0        ...           -1  1.21899986267032  1.47299957275415   \n",
       "1        ...           -1  1.34599971771224                -1   \n",
       "2        ...           -1  1.32099914550831  1.49899959564243   \n",
       "3        ...           -1  1.29499912262003  1.51099967956562   \n",
       "4        ...           -1  1.34599971771224                -1   \n",
       "\n",
       "             dvht16           dvrwt07           dvrwt11           dvrwt16  \\\n",
       "0  1.59999942779607  110.347991943347  98.1929931640604  105.055999755876   \n",
       "1                -1   90.865997314449                -1                -1   \n",
       "2  1.87999916076665  87.9599914550983  96.4049987792867   89.382995605487   \n",
       "3  1.62999916076665   105.16198730471  111.588989257796  132.054992675766   \n",
       "4                -1  129.382995605487                -1                -1   \n",
       "\n",
       "             dvwt07            dvwt11            dvwt16  \n",
       "0  25.8549957275385  37.6489868164152  56.0199890136717  \n",
       "1  26.3089904785155                -1                -1  \n",
       "2  24.4939880371087  38.1019897460905  66.6799926757659  \n",
       "3   28.122985839843  45.8139953613169  72.7999877929584  \n",
       "4  37.6489868164152                -1                -1  \n",
       "\n",
       "[5 rows x 1765 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load and inspect the ncds data\n",
    "ncds_data = pd.read_csv('datasets/ncds0123.txt', delimiter='\\t', low_memory=False)\n",
    "# Print shapes\n",
    "print \"Shape of data:\", ncds_data.shape\n",
    "ncds_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data: (16990, 54)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NCDSID</th>\n",
       "      <th>N622</th>\n",
       "      <th>BSTATUS</th>\n",
       "      <th>POD</th>\n",
       "      <th>BOOKING</th>\n",
       "      <th>PLANC</th>\n",
       "      <th>DIASTOL</th>\n",
       "      <th>MAXDBP</th>\n",
       "      <th>ALBECL</th>\n",
       "      <th>XRAY</th>\n",
       "      <th>...</th>\n",
       "      <th>DTB8</th>\n",
       "      <th>DTB9</th>\n",
       "      <th>DTB10</th>\n",
       "      <th>ILLNESS</th>\n",
       "      <th>MOD</th>\n",
       "      <th>TOD</th>\n",
       "      <th>AAD</th>\n",
       "      <th>SBNND</th>\n",
       "      <th>PLCWGT</th>\n",
       "      <th>TABLE62</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>N10001N</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-2</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>N10002P</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-2</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>N10003Q</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-2</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>N10004R</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>-8</td>\n",
       "      <td>-8</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-2</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>N10005S</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-2</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 54 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    NCDSID  N622  BSTATUS POD BOOKING PLANC DIASTOL MAXDBP ALBECL XRAY  \\\n",
       "0  N10001N     2        0   8       8     2       1      1      0    0   \n",
       "1  N10002P     1        0   2       0     4       4      3      0    0   \n",
       "2  N10003Q     1        0   8       8     2       1      3      0    0   \n",
       "3  N10004R     2        0   8       8     2       1     -8     -8    1   \n",
       "4  N10005S     2        0   8       8     2       1      3      0    1   \n",
       "\n",
       "    ...   DTB8 DTB9 DTB10 ILLNESS MOD TOD AAD SBNND PLCWGT TABLE62  \n",
       "0   ...      0    0     0       0   0  -1  -1    -1     -2      -1  \n",
       "1   ...      0    0     0       0   0  -1  -1    -1     -2      -1  \n",
       "2   ...      0    0     1       3   0  -1  -1    -1     -2      -1  \n",
       "3   ...      1    0     0      -1   0  -1  -1    -1     -2      -1  \n",
       "4   ...      0    0     0       0   0  -1  -1    -1     -2      -1  \n",
       "\n",
       "[5 rows x 54 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load and inspect the pms additions data\n",
    "ncds_pms_data = pd.read_csv('datasets/ncds_pms_additionals.txt', delimiter='\\t', low_memory=False)\n",
    "# Print shapes\n",
    "print \"Shape of data:\", ncds_pms_data.shape\n",
    "ncds_pms_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data: (18558, 18)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NCDSID</th>\n",
       "      <th>N622</th>\n",
       "      <th>BSTATUS</th>\n",
       "      <th>COBIRTH</th>\n",
       "      <th>MULTIPNO</th>\n",
       "      <th>MULTCODE</th>\n",
       "      <th>ETHNICID</th>\n",
       "      <th>OUTCME00</th>\n",
       "      <th>OUTCME01</th>\n",
       "      <th>OUTCME02</th>\n",
       "      <th>OUTCME03</th>\n",
       "      <th>OUTCME04</th>\n",
       "      <th>OUTCME05</th>\n",
       "      <th>OUTCME06</th>\n",
       "      <th>OUTCMEBM</th>\n",
       "      <th>OUTCME07</th>\n",
       "      <th>OUTCME08</th>\n",
       "      <th>OUTCME09</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>N10001N</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>N10002P</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>N10003Q</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>N10004R</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>N10005S</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    NCDSID  N622  BSTATUS  COBIRTH  MULTIPNO  MULTCODE  ETHNICID  OUTCME00  \\\n",
       "0  N10001N     2        0        1        -1        -1         1         1   \n",
       "1  N10002P     1        0        1        -1        -1         1         1   \n",
       "2  N10003Q     1        0        1        -1        -1         1         1   \n",
       "3  N10004R     2        0        1        -1        -1         1         1   \n",
       "4  N10005S     2        0        2        -1        -1         5         1   \n",
       "\n",
       "   OUTCME01  OUTCME02  OUTCME03  OUTCME04  OUTCME05  OUTCME06  OUTCMEBM  \\\n",
       "0         1         1         1         1         1         1         1   \n",
       "1         1         1         1         1         1         1         1   \n",
       "2         1         1         1         7         7         7         6   \n",
       "3         1         1         1         1         1         1         1   \n",
       "4         1         1         1         2         2         2         6   \n",
       "\n",
       "   OUTCME07  OUTCME08  OUTCME09  \n",
       "0         1         1         1  \n",
       "1         1         1         1  \n",
       "2         7         7         7  \n",
       "3         2         2         3  \n",
       "4         6         6         6  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load and inspect the response data\n",
    "ncds_response_data = pd.read_csv('datasets/ncds_response.txt', delimiter='\\t', low_memory=False)\n",
    "# Print shapes\n",
    "print \"Shape of data:\", ncds_response_data.shape\n",
    "ncds_response_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1.1: Understand the data:\n",
    "\n",
    "1. Explore the data\n",
    "2. Understand the predictors, what they mean in real life\n",
    "3. Understand the values of each predictors\n",
    "4. Join appropriate datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Columns the help us identify if the patient has epilepsy\n",
    "epil_columns = [\"n390\",\"n391\",\"n392\",\"n415\", \"n1842\", \"n1307\", \"n1308\", \"n1309\", \"n1314\", \"n1317\", \"n1477\", \"n1478\", \"n1479\", \"n2416\", \"n2663\", \"n2664\", \"n2665\"\n",
    "                , \"n2666\", \"n2667\", \"n1893\", \"n1894\", \"n1895\", \"n1904\", \"n1910\", 'n1817', 'n1818',  'n1394', 'n1502', 'n2615', 'n2616']\n",
    "\n",
    "def evaluate_data(df):\n",
    "    # Check for range of unique values for the train data\n",
    "    for i in range(df.shape[1]):\n",
    "        vals = np.unique(df.iloc[:, i])\n",
    "        if len(vals) < 15:\n",
    "            print '(Categorical) {} unique values - {}: {}'.format(len(vals), df.columns[i], vals)\n",
    "        else:\n",
    "            print '(Continuous) range of values - ', df.columns[i], ': {} to {}'.format(df.iloc[:, i].min(), df.iloc[:, i].max())\n",
    "\n",
    "def evaluate_epil_columns(df):\n",
    "    for column in epil_columns:\n",
    "        vals = np.unique(df[column])\n",
    "        if len(vals) < 15:\n",
    "            print '(Categorical) {} unique values - {}: {}'.format(len(vals), column, vals)\n",
    "        else:\n",
    "            print '(Continuous) range of values - ', column, ': {} to {}'.format(df[column].min(), df[column].max())\n",
    "\n",
    "\n",
    "def columns_with_null(df):\n",
    "    for column in df.columns:\n",
    "        df_missing = df[df[column].isnull()]\n",
    "        count = 0\n",
    "        if df_missing.shape[0] > 0:\n",
    "            print \"Predictor \" , column, \" contain null values / Count = \" ,df_missing.shape[0]\n",
    "            count = count +1\n",
    "    print \"Total number of columns with null:\",count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data: (18558, 1837)\n"
     ]
    }
   ],
   "source": [
    "# Join datasets\n",
    "ncds_merged_data = pd.merge(left=ncds_data,right=ncds_pms_data,how='left',left_on='ncdsid',right_on='NCDSID')\n",
    "ncds_merged_data = pd.merge(left=ncds_merged_data,right=ncds_response_data,how='left',left_on='ncdsid',right_on='NCDSID')\n",
    "print \"Shape of data:\", ncds_merged_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Evalute the ncds data\n",
    "#evaluate_data(ncds_merged_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1.2: Handle missing data:\n",
    "\n",
    "Are there any missing values, if there are:\n",
    "1. Can we impute them based on some algorithm\n",
    "2. Remove or ignore them\n",
    "3. Assume values based on common sense or prior knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Remove spaces from data\n",
    "def convert_spaces_to_null(data):\n",
    "    data = data.replace([' '],[None]) \n",
    "    return data\n",
    "\n",
    "def fill_with_median(x_fill):\n",
    "    x_fill = x_fill.groupby(x_fill.columns, axis = 1).transform(lambda x: x.fillna(x.median()))\n",
    "    return x_fill\n",
    "\n",
    "def fill_with_mean(x_fill):\n",
    "    x_fill = x_fill.groupby(x_fill.columns, axis = 1).transform(lambda x: x.fillna(x.mean()))\n",
    "    return x_fill\n",
    "\n",
    "def fill_pms_columns(x_fill):\n",
    "    for index, row in x_fill.iterrows():\n",
    "#         # 0-3D Sex of child \n",
    "#         if pd.isnull(row[\"N622\"]):\n",
    "#             x_fill.set_value(index, 'N622', -1.0)\n",
    "        # Reconciled Birth Status  \n",
    "#         if pd.isnull(row[\"BSTATUS\"]):\n",
    "#             x_fill.set_value(index, 'BSTATUS', 0)\n",
    "        # Q6:Place of Delivery  \n",
    "        if pd.isnull(row[\"POD\"]):\n",
    "            x_fill.set_value(index, 'POD', 5.0)\n",
    "        # Q26b: Booking In place  \n",
    "        if pd.isnull(row[\"BOOKING\"]):\n",
    "            x_fill.set_value(index, 'BOOKING', 3.0)\n",
    "        # Q21b: Place of Antenatal care  \n",
    "        if pd.isnull(row[\"PLANC\"]):\n",
    "            x_fill.set_value(index, 'PLANC', -8.0)\n",
    "        # Q29a: Diastolic Blood Pressure  \n",
    "        if pd.isnull(row[\"DIASTOL\"]):\n",
    "            x_fill.set_value(index, 'DIASTOL', -2.0)\n",
    "        # Q29b: Maximum Diatolic Blood Pressure \n",
    "        if pd.isnull(row[\"MAXDBP\"]):\n",
    "            x_fill.set_value(index, 'MAXDBP', -2.0)\n",
    "        # Q31: Albuminuria and Eclampsia\n",
    "        if pd.isnull(row[\"ALBECL\"]):\n",
    "            x_fill.set_value(index, 'ALBECL', -8.0)\n",
    "        # Q36: X-Ray given\n",
    "        if pd.isnull(row[\"XRAY\"]):\n",
    "            x_fill.set_value(index, 'XRAY', -8.0)\n",
    "        # Q37: Obstetric, pregnancy abnormality - No information\n",
    "        if pd.isnull(row[\"ABNORM0X\"]):\n",
    "            x_fill.set_value(index, 'ABNORM0X', -2.0)\n",
    "        # Q37: No Obstetric, pregnancy abnormality\n",
    "        if pd.isnull(row[\"ABNORM00\"]):\n",
    "            x_fill.set_value(index, 'ABNORM00', -2.0)\n",
    "        # Q37: Obstetric, pregnancy abnormality - Diabetes\n",
    "        if pd.isnull(row[\"ABNORM01\"]):\n",
    "            x_fill.set_value(index, 'ABNORM01', -2.0)\n",
    "        # Q37: Obstetric, pregnancy abnormality - Heart \n",
    "        if pd.isnull(row[\"ABNORM02\"]):\n",
    "            x_fill.set_value(index, 'ABNORM02', -2.0)\n",
    "        # Q37: Obstetric, pregnancy abnormality - Active TB \n",
    "        if pd.isnull(row[\"ABNORM03\"]):\n",
    "            x_fill.set_value(index, 'ABNORM03', -2.0)\n",
    "        # Q37: Obstetric, pregnancy abnormality - influenza \n",
    "        if pd.isnull(row[\"ABNORM04\"]):\n",
    "            x_fill.set_value(index, 'ABNORM04', -2.0)\n",
    "        # Q37: Obstetric, pregnancy abnormality - German Measles \n",
    "        if pd.isnull(row[\"ABNORM05\"]):\n",
    "            x_fill.set_value(index, 'ABNORM05', -2.0)\n",
    "        # Q37: Obstetric, pregnancy abnormality - Disproportion \n",
    "        if pd.isnull(row[\"ABNORM06\"]):\n",
    "            x_fill.set_value(index, 'ABNORM06', -2.0)\n",
    "        # Q37: Obstetric, pregnancy abnormality - External version \n",
    "        if pd.isnull(row[\"ABNORM07\"]):\n",
    "            x_fill.set_value(index, 'ABNORM07', -2.0)\n",
    "        # Q37: Obstetric, pregnancy abnormality - Epilepsy \n",
    "        if pd.isnull(row[\"ABNORM08\"]):\n",
    "            x_fill.set_value(index, 'ABNORM08', -2.0)\n",
    "        # Q37: Obstetric, pregnancy abnormality - Other \n",
    "        if pd.isnull(row[\"ABNORM09\"]):\n",
    "            x_fill.set_value(index, 'ABNORM09', -2.0)\n",
    "        # Q37: Bleeding in Pregnancy and before delivery \n",
    "        if pd.isnull(row[\"BLEED\"]):\n",
    "            x_fill.set_value(index, 'BLEED', -1.0)\n",
    "        # Q38a: Admission to hospital  \n",
    "        if pd.isnull(row[\"AD2HOSP\"]):\n",
    "            x_fill.set_value(index, 'AD2HOSP', -1.0)\n",
    "        # Q39: Type of Labour or Delivery Admission (Hospital)  \n",
    "        if pd.isnull(row[\"ADTYPE\"]):\n",
    "            x_fill.set_value(index, 'ADTYPE', -1.0)\n",
    "        # Q44: Presenting Part  \n",
    "        if pd.isnull(row[\"PRESENT\"]):\n",
    "            x_fill.set_value(index, 'PRESENT', -1.0)\n",
    "        # Q49a: No drugs of this type  \n",
    "        if pd.isnull(row[\"LDRUG00\"]):\n",
    "            x_fill.set_value(index, 'LDRUG00', -2.0)\n",
    "        # Q49a: Chloral, Welldorm  \n",
    "        if pd.isnull(row[\"LDRUG01\"]):\n",
    "            x_fill.set_value(index, 'LDRUG01', -2.0)\n",
    "        # Q49a: Barbiturate   \n",
    "        if pd.isnull(row[\"LDRUG02\"]):\n",
    "            x_fill.set_value(index, 'LDRUG02', -2.0)\n",
    "        # Q49a: Heroin   \n",
    "        if pd.isnull(row[\"LDRUG03\"]):\n",
    "            x_fill.set_value(index, 'LDRUG03', -2.0)\n",
    "        # Q49a: Largactil (chlorpomazine)   \n",
    "        if pd.isnull(row[\"LDRUG04\"]):\n",
    "            x_fill.set_value(index, 'LDRUG04', -2.0)\n",
    "        # Q49a: Sparine (promazine)    \n",
    "        if pd.isnull(row[\"LDRUG05\"]):\n",
    "            x_fill.set_value(index, 'LDRUG05', -2.0)\n",
    "        # Q49a: Phenergan (promethazine)    \n",
    "        if pd.isnull(row[\"LDRUG06\"]):\n",
    "            x_fill.set_value(index, 'LDRUG06', -2.0)\n",
    "        # Q49a: Doriden    \n",
    "        if pd.isnull(row[\"LDRUG07\"]):\n",
    "            x_fill.set_value(index, 'LDRUG07', -2.0)\n",
    "        # Q49a: Oblivon    \n",
    "        if pd.isnull(row[\"LDRUG08\"]):\n",
    "            x_fill.set_value(index, 'LDRUG08', -2.0)\n",
    "        # Q49a: Other    \n",
    "        if pd.isnull(row[\"LDRUG09\"]):\n",
    "            x_fill.set_value(index, 'LDRUG09', -2.0)\n",
    "        # Q50: Anaesthetic    \n",
    "        if pd.isnull(row[\"ATHETIC\"]):\n",
    "            x_fill.set_value(index, 'ATHETIC', -2.0)\n",
    "        # Q55: Resuscitation    \n",
    "        if pd.isnull(row[\"RESUS\"]):\n",
    "            x_fill.set_value(index, 'RESUS', -2.0)\n",
    "        # Q56: Drugs to baby (None)      \n",
    "        if pd.isnull(row[\"DTB1\"]):\n",
    "            x_fill.set_value(index, 'DTB1', -2.0)\n",
    "        # Q56: Drugs to baby (Coranine)       \n",
    "        if pd.isnull(row[\"DTB2\"]):\n",
    "            x_fill.set_value(index, 'DTB2', -2.0)\n",
    "        # Q56: Drugs to baby (Lobeline)       \n",
    "        if pd.isnull(row[\"DTB3\"]):\n",
    "            x_fill.set_value(index, 'DTB3', -2.0)\n",
    "        # Q56: Drugs to baby (Sedatives)       \n",
    "        if pd.isnull(row[\"DTB4\"]):\n",
    "            x_fill.set_value(index, 'DTB4', -2.0)\n",
    "        # Q56: Drugs to baby (Antagonists, nalorphine, levalorfan)       \n",
    "        if pd.isnull(row[\"DTB5\"]):\n",
    "            x_fill.set_value(index, 'DTB5', -2.0)\n",
    "        # Q56: Drugs to baby (Synkavit, Vikastab)      \n",
    "        if pd.isnull(row[\"DTB6\"]):\n",
    "            x_fill.set_value(index, 'DTB6', -2.0)\n",
    "        # Q56: Drugs to baby (Sulphonamides)      \n",
    "        if pd.isnull(row[\"DTB7\"]):\n",
    "            x_fill.set_value(index, 'DTB7', -2.0)\n",
    "        # Q56: Drugs to baby (Penicilin)      \n",
    "        if pd.isnull(row[\"DTB8\"]):\n",
    "            x_fill.set_value(index, 'DTB8', -2.0)\n",
    "        # Q56: Drugs to baby (Streptomycin)       \n",
    "        if pd.isnull(row[\"DTB9\"]):\n",
    "            x_fill.set_value(index, 'DTB9', -2.0)\n",
    "        # Q56: Drugs to baby (Other antibiotics)       \n",
    "        if pd.isnull(row[\"DTB10\"]):\n",
    "            x_fill.set_value(index, 'DTB10', -2.0)\n",
    "        # Q59: Baby's Illness       \n",
    "        if pd.isnull(row[\"ILLNESS\"]):\n",
    "            x_fill.set_value(index, 'ILLNESS', -1.0)\n",
    "        # Q61: Month of Death       \n",
    "        if pd.isnull(row[\"MOD\"]):\n",
    "            x_fill.set_value(index, 'MOD', 0.0)\n",
    "        # Q61: Time of death        \n",
    "        if pd.isnull(row[\"TOD\"]):\n",
    "            x_fill.set_value(index, 'TOD', -1.0)\n",
    "        # Q61: Age at Death        \n",
    "        if pd.isnull(row[\"AAD\"]):\n",
    "            x_fill.set_value(index, 'AAD', -1.0)\n",
    "        # Q61: Still Birth or Neo-natal Death (Dervied)         \n",
    "        if pd.isnull(row[\"SBNND\"]):\n",
    "            x_fill.set_value(index, 'SBNND', -1.0)\n",
    "        # Placental Weight         \n",
    "        if pd.isnull(row[\"PLCWGT\"]):\n",
    "            x_fill.set_value(index, 'PLCWGT', -2.0)\n",
    "        # Time of death for still births and neonatal deaths (Table 62)          \n",
    "        if pd.isnull(row[\"TABLE62\"]):\n",
    "            x_fill.set_value(index, 'TABLE62', -1.0)\n",
    "    return x_fill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make a copy\n",
    "ncds_data_clean = ncds_merged_data.copy() \n",
    "\n",
    "# drop the ID columns\n",
    "ncds_data_clean =  ncds_data_clean.drop([\"ncdsid\",\"NCDSID_x\",\"NCDSID_y\"],axis=1)\n",
    "# Drop other duplicate columns\n",
    "ncds_data_clean =  ncds_data_clean.drop([\"N622_x\",\"BSTATUS_x\"],axis=1)\n",
    "\n",
    "\n",
    "# Convert spaces in the data to nulls\n",
    "ncds_data_clean = convert_spaces_to_null(ncds_data_clean)\n",
    "\n",
    "# Convert all columns to float\n",
    "for column in ncds_data_clean.columns:\n",
    "    ncds_data_clean[column] = ncds_data_clean[column].astype(float)\n",
    "\n",
    "# Impute missing data from joined columns using default values\n",
    "ncds_data_clean = fill_pms_columns(ncds_data_clean)\n",
    "\n",
    "# Impute missing data with median values\n",
    "ncds_data_clean = fill_with_median(ncds_data_clean)\n",
    "\n",
    "# Impute missing data with mean values - there are some columns we cannot impute with median\n",
    "ncds_data_clean = fill_with_mean(ncds_data_clean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of columns with null: 0\n"
     ]
    }
   ],
   "source": [
    "# Get the columns which have null data\n",
    "columns_with_null(ncds_data_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1.3: Identify Epilepsy Records:\n",
    "\n",
    "In our data a patient is assumed to be epileptic is one or more conditions are satisified in the dataset. We need to check all the conditions in the data and determine if the patient is epileptic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Identify if patient has epilepsy\n",
    "ncds_data_clean[\"epileptic\"] = 0\n",
    "for index, row in ncds_data_clean.iterrows():\n",
    "    # 1M Reason for Special Education MC1:3\n",
    "    if row[\"n390\"] == 10.0:\n",
    "        ncds_data_clean.set_value(index, 'epileptic', 1)\n",
    "    # 1M Reason for Special Education MC2:3\n",
    "    if row[\"n391\"] == 10.0:\n",
    "        ncds_data_clean.set_value(index, 'epileptic', 1)\n",
    "    # 1M Reason for Special Education MC2:3\n",
    "    if row[\"n392\"] == 10.0:\n",
    "        ncds_data_clean.set_value(index, 'epileptic', 1)\n",
    "    # 1M Epileptic condition\n",
    "    if row[\"n415\"] >= 3.0 :\n",
    "        ncds_data_clean.set_value(index, 'epileptic', 1)\n",
    "    # 12D Epilepsy identification\n",
    "    if row[\"n1842\"] == 5.0 :\n",
    "        ncds_data_clean.set_value(index, 'epileptic', 1)\n",
    "    # 2P Has child had epilepsy attacks-MC 1:3\n",
    "    if (row[\"n1307\"] >= 1.0 and row[\"n1307\"] <= 5.0):\n",
    "        ncds_data_clean.set_value(index, 'epileptic', 1)\n",
    "    # 2P Has child had epilepsy attacks-MC 2:3\n",
    "    if (row[\"n1308\"] >= 1.0 and row[\"n1308\"] <= 5.0):\n",
    "        ncds_data_clean.set_value(index, 'epileptic', 1)\n",
    "    # 2P Has child had epilepsy attacks-MC 3:3\n",
    "    if (row[\"n1309\"] >= 1.0 and row[\"n1309\"] <= 5.0):\n",
    "        ncds_data_clean.set_value(index, 'epileptic', 1)\n",
    "    # 2P Age at most recent epilepsy attack\n",
    "    if (row[\"n1314\"] >= 0.0):\n",
    "        ncds_data_clean.set_value(index, 'epileptic', 1)\n",
    "    # 2P Age at 1st epilepsy attack\n",
    "    if (row[\"n1317\"] >= 0.0):\n",
    "        ncds_data_clean.set_value(index, 'epileptic', 1)\n",
    "    # 2M Reason for special education - MC1:3\n",
    "    if row[\"n1477\"] == 7.0:\n",
    "        ncds_data_clean.set_value(index, 'epileptic', 1)\n",
    "    # 2M Reason for special education - MC2:3\n",
    "    if row[\"n1478\"] == 7.0:\n",
    "        ncds_data_clean.set_value(index, 'epileptic', 1)\n",
    "    # 2M Reason for special education - MC3:3\n",
    "    if row[\"n1479\"] == 7.0:\n",
    "        ncds_data_clean.set_value(index, 'epileptic', 1)\n",
    "    # 3P Type hcap for which will require help\n",
    "    if row[\"n2416\"] == 7.0:\n",
    "        ncds_data_clean.set_value(index, 'epileptic', 1)\n",
    "    # 3P Nature of child-s disability-MC 1:5\n",
    "    if row[\"n2663\"] == 7.0:\n",
    "        ncds_data_clean.set_value(index, 'epileptic', 1)\n",
    "    # 3P Nature of child-s disability-MC 2:5\n",
    "    if row[\"n2664\"] == 7.0:\n",
    "        ncds_data_clean.set_value(index, 'epileptic', 1)\n",
    "    # 3P Nature of child-s disability-MC 3:5\n",
    "    if row[\"n2665\"] == 7.0:\n",
    "        ncds_data_clean.set_value(index, 'epileptic', 1)\n",
    "    # 3P Nature of child-s disability-MC 4:5\n",
    "    if row[\"n2666\"] == 7.0:\n",
    "        ncds_data_clean.set_value(index, 'epileptic', 1)\n",
    "    # 3P Nature of child-s disability-MC 5:5\n",
    "    if row[\"n2667\"] == 7.0:\n",
    "        ncds_data_clean.set_value(index, 'epileptic', 1)\n",
    "    # 3M Category of child's handicap MC1:3\n",
    "    if row[\"n1893\"] == 8.0:\n",
    "        ncds_data_clean.set_value(index, 'epileptic', 1)\n",
    "    # 3M Category of child's handicap MC2:3\n",
    "    if row[\"n1894\"] == 8.0:\n",
    "        ncds_data_clean.set_value(index, 'epileptic', 1)\n",
    "    # 3M Category of child's handicap MC3:3\n",
    "    if row[\"n1895\"] == 8.0:\n",
    "        ncds_data_clean.set_value(index, 'epileptic', 1)\n",
    "    # 3M Reason for hosp admiss last 12 mnths\n",
    "    if row[\"n1904\"] == 17.0:\n",
    "        ncds_data_clean.set_value(index, 'epileptic', 1)\n",
    "    # 3M Reason hosp outpatient last yr\n",
    "    if row[\"n1910\"] == 17.0:\n",
    "        ncds_data_clean.set_value(index, 'epileptic', 1)\n",
    "    # 3M Epilepsy\n",
    "#     if row[\"n2032\"] >= 1.0:\n",
    "#         ncds_data_clean.set_value(index, 'epileptic', 1)\n",
    "    \n",
    "    # New columns \n",
    "    \n",
    "    # 1D Defects found in NCDS1 sample-MC 1:4\n",
    "    if row[\"n1817\"] > 0.0:\n",
    "        ncds_data_clean.set_value(index, 'epileptic', 1)\n",
    "    # 1D Defects found in NCDS1 sample-MC 2:4\n",
    "    if row[\"n1818\"] > 0.0:\n",
    "        ncds_data_clean.set_value(index, 'epileptic', 1)\n",
    "    # 1D Defects found in NCDS1 sample-MC 3:4\n",
    "    if row[\"n1819\"] > 0.0:\n",
    "        ncds_data_clean.set_value(index, 'epileptic', 1)\n",
    "    # 2P Ever seen specialist-convulsions,fits\n",
    "    if row[\"n1394\"] == 5.0:\n",
    "        ncds_data_clean.set_value(index, 'epileptic', 1)\n",
    "    # 2M Has child ever had convulsions\n",
    "    if any(row[\"n1502\"] == s for s in [2.0,3.0,4.0]):\n",
    "        ncds_data_clean.set_value(index, 'epileptic', 1)\n",
    "    # 3P When convulsions,fits 1st occured\n",
    "    if any(row[\"n2615\"] == s for s in [1.0,2.0,3.0,4.0,5.0,6.0]):\n",
    "        ncds_data_clean.set_value(index, 'epileptic', 1)\n",
    "    # 3P Convulsions-most recent occurrence\n",
    "    if any(row[\"n2616\"] == s for s in [1.0,2.0,3.0,4.0,5.0,6.0,7.0]):\n",
    "        ncds_data_clean.set_value(index, 'epileptic', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with epilepsy 1609\n"
     ]
    }
   ],
   "source": [
    "print \"Number of rows with epilepsy\",ncds_data_clean[ncds_data_clean[\"epileptic\"] == 1].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of dataset:  (18558, 1803)\n"
     ]
    }
   ],
   "source": [
    "# Remove the epilepsy columns from the data\n",
    "ncds_data_no_indicators=ncds_data_clean.copy()\n",
    "ncds_data_no_indicators.drop(epil_columns,inplace=True,axis=1)\n",
    "print \"Shape of dataset: \" , ncds_data_no_indicators.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1.4: Split data into train and test:\n",
    "\n",
    "Split our dataset into train and test and analyze the splits. We can explore and verify the matrix of classes to check if our data is balanced. If the class is Imbalanced we will need to do any of the following:\n",
    "1. Over sample\n",
    "2. Under sample\n",
    "3. Over weight\n",
    "4. Adjust class weights in model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data:  (11134, 1802)\n",
      "Test data:  (7424, 1802)\n",
      "Train class 0: 10159, train class 1: 975\n",
      "Test class 0: 6790, test class 1: 634\n"
     ]
    }
   ],
   "source": [
    "x = ncds_data_no_indicators.values[:, :-1]\n",
    "y = ncds_data_no_indicators.values[:, -1]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.4, random_state=42)\n",
    "\n",
    "#Print some useful info for our test, train sets\n",
    "print 'Train data: ', x_train.shape\n",
    "print 'Test data: ', x_test.shape\n",
    "print 'Train class 0: {}, train class 1: {}'.format(len(y_train[y_train == 0]), len(y_train[y_train == 1]))\n",
    "print 'Test class 0: {}, test class 1: {}'.format(len(y_test[y_test == 0]), len(y_test[y_test == 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Feature Selection\n",
    "\n",
    "From the merged datasets we can see we have over 1800 features. Going through the 1800 would be a very time consuming task so let us apply some algorithims to find the best features that we can use to build the model. In our exploration phase we did use PCA to find a subset of components but chose not to use those components in our base models. The exploration phase can be seen [here](ExploreData.ipynb). However we may chose to use PCA during model tuning and evaluating model performance phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Features:\n",
      "['n1827' 'n604' 'n39' 'n1263' 'n1400' 'n1399' 'n1453' 'n1476' 'n825'\n",
      " 'n2598' 'n1896' 'n1898' 'dvht07' 'OUTCME01' 'OUTCME02']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/envs/py27/lib/python2.7/site-packages/pandas/indexes/base.py:1275: VisibleDeprecationWarning: boolean index did not match indexed array along dimension 0; dimension is 1803 but corresponding boolean dimension is 1802\n",
      "  result = getitem(key)\n"
     ]
    }
   ],
   "source": [
    "# Best features\n",
    "num_of_features = 15\n",
    "features = fs.SelectKBest(fs.f_regression, k=num_of_features) #k is number of features.\n",
    "features.fit(x_train, y_train)\n",
    "\n",
    "selected_features = features.get_support()\n",
    "print \"Selected Features:\"\n",
    "selected_features_columns =  ncds_data_no_indicators.columns[selected_features].values\n",
    "print selected_features_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Build Base Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function for computing the accuracy a given model on the entire test set,\n",
    "# the accuracy on class 0 in the test set\n",
    "# and the accuracy on class 1\n",
    "score = lambda model, x_test, y_test: pd.Series([model.score(x_test, y_test), \n",
    "                                                 model.score(x_test[y_test==0], y_test[y_test==0]),\n",
    "                                                 model.score(x_test[y_test==1], y_test[y_test==1])],\n",
    "                                                index=['overall accuracy', 'accuracy on class 0', 'accuracy on class 1'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data:  (11134, 15)\n",
      "Test data:  (7424, 15)\n",
      "Train class 0: 10159, train class 1: 975\n",
      "Test class 0: 6790, test class 1: 634\n"
     ]
    }
   ],
   "source": [
    "# Split data for selected features only\n",
    "#x = ncds_data_no_indicators.values[:, selected_features]\n",
    "x = ncds_data_no_indicators[selected_features_columns].values[:,:]\n",
    "y = ncds_data_no_indicators.values[:, -1]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.4, random_state=42)\n",
    "\n",
    "#Print some useful info for our test, train sets\n",
    "print 'Train data: ', x_train.shape\n",
    "print 'Test data: ', x_test.shape\n",
    "print 'Train class 0: {}, train class 1: {}'.format(len(y_train[y_train == 0]), len(y_train[y_train == 1]))\n",
    "print 'Test class 0: {}, test class 1: {}'.format(len(y_test[y_test == 0]), len(y_test[y_test == 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3.1: Linear Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear regression:\n",
      "overall accuracy       0.050431\n",
      "accuracy on class 0    0.000000\n",
      "accuracy on class 1    0.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Linear regression\n",
    "linear = LinearRegression()\n",
    "linear.fit(x_train, y_train)\n",
    "linear_scores = score(linear, x_test, y_test)\n",
    "print \"Linear regression:\"\n",
    "print linear_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3.2: Logistic Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression (Unweighted):\n",
      "overall accuracy       0.915948\n",
      "accuracy on class 0    0.997791\n",
      "accuracy on class 1    0.039432\n",
      "dtype: float64\n",
      "Logistic regression (Weighted):\n",
      "overall accuracy       0.623788\n",
      "accuracy on class 0    0.625626\n",
      "accuracy on class 1    0.604101\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Unweighted logistic regression\n",
    "unweighted_logistic = LogisticRegression()\n",
    "unweighted_logistic.fit(x_train, y_train)\n",
    "unweighted_log_scores = score(unweighted_logistic, x_test, y_test)\n",
    "\n",
    "# Weighted logistic regression\n",
    "weighted_logistic = LogisticRegression(class_weight='balanced')\n",
    "weighted_logistic.fit(x_train, y_train)\n",
    "weighted_log_scores = score(weighted_logistic, x_test, y_test)\n",
    "\n",
    "print \"Logistic regression (Unweighted):\"\n",
    "print unweighted_log_scores\n",
    "print \"Logistic regression (Weighted):\"\n",
    "print weighted_log_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3.3: Linear Discriminant Analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA:\n",
      "overall accuracy       0.912177\n",
      "accuracy on class 0    0.988954\n",
      "accuracy on class 1    0.089905\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# LDA\n",
    "lda = LDA()\n",
    "lda.fit(x_train, y_train)\n",
    "lda_scores = score(lda, x_test, y_test)\n",
    "\n",
    "print \"LDA:\"\n",
    "print lda_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3.4: Other Models:\n",
    "\n",
    "Let us build some more baseline models using other techniques and compare to the ones above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>knn</th>\n",
       "      <th>lda</th>\n",
       "      <th>linear</th>\n",
       "      <th>qda</th>\n",
       "      <th>rf</th>\n",
       "      <th>svc</th>\n",
       "      <th>tree</th>\n",
       "      <th>unweighted logistic</th>\n",
       "      <th>weighted logistic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>overall accuracy</th>\n",
       "      <td>0.909752</td>\n",
       "      <td>0.912177</td>\n",
       "      <td>0.050431</td>\n",
       "      <td>0.851832</td>\n",
       "      <td>0.799300</td>\n",
       "      <td>0.715383</td>\n",
       "      <td>0.895609</td>\n",
       "      <td>0.915948</td>\n",
       "      <td>0.623788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy on class 0</th>\n",
       "      <td>0.988218</td>\n",
       "      <td>0.988954</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.911487</td>\n",
       "      <td>0.856701</td>\n",
       "      <td>0.737113</td>\n",
       "      <td>0.964654</td>\n",
       "      <td>0.997791</td>\n",
       "      <td>0.625626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy on class 1</th>\n",
       "      <td>0.069401</td>\n",
       "      <td>0.089905</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.212934</td>\n",
       "      <td>0.184543</td>\n",
       "      <td>0.482650</td>\n",
       "      <td>0.156151</td>\n",
       "      <td>0.039432</td>\n",
       "      <td>0.604101</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          knn       lda    linear       qda        rf  \\\n",
       "overall accuracy     0.909752  0.912177  0.050431  0.851832  0.799300   \n",
       "accuracy on class 0  0.988218  0.988954  0.000000  0.911487  0.856701   \n",
       "accuracy on class 1  0.069401  0.089905  0.000000  0.212934  0.184543   \n",
       "\n",
       "                          svc      tree  unweighted logistic  \\\n",
       "overall accuracy     0.715383  0.895609             0.915948   \n",
       "accuracy on class 0  0.737113  0.964654             0.997791   \n",
       "accuracy on class 1  0.482650  0.156151             0.039432   \n",
       "\n",
       "                     weighted logistic  \n",
       "overall accuracy              0.623788  \n",
       "accuracy on class 0           0.625626  \n",
       "accuracy on class 1           0.604101  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# KNN\n",
    "knn = KNN()\n",
    "knn.fit(x_train, y_train)\n",
    "knn_scores = score(knn, x_test, y_test)\n",
    "\n",
    "#QDA\n",
    "qda = QDA()\n",
    "qda.fit(x_train, y_train)\n",
    "qda_scores = score(qda, x_test, y_test)\n",
    "\n",
    "#Decision Tree\n",
    "tree = DecisionTree()\n",
    "tree.fit(x_train, y_train)\n",
    "tree_scores = score(tree, x_test, y_test)\n",
    "\n",
    "#Random Forest\n",
    "rf = RandomForest(class_weight='balanced')\n",
    "rf.fit(x_train, y_train)\n",
    "rf_scores = score(rf, x_test, y_test)\n",
    "\n",
    "# SVC\n",
    "svc = SVC(probability=True,class_weight='balanced')\n",
    "svc.fit(x_train, y_train)\n",
    "svc_scores = score(svc, x_test, y_test)\n",
    "\n",
    "#Score Dataframe\n",
    "score_df = pd.DataFrame({'linear':linear_scores,\n",
    "                         'unweighted logistic': unweighted_log_scores,\n",
    "                         'weighted logistic': weighted_log_scores,\n",
    "                         'lda': lda_scores,\n",
    "                         'qda': qda_scores,\n",
    "                        'knn': knn_scores,\n",
    "                         'tree': tree_scores,\n",
    "                         'rf': rf_scores,'svc':svc_scores})\n",
    "score_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Feature Selection by Demographic:\n",
    "\n",
    "Let's group the data by demographics and perform a feature selection on each individual group\n",
    "1. Sex\n",
    "2. Country of birth\n",
    "3. Ethnic group "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_features_by_group(column,list_of_group):\n",
    "    for i in list_of_group:\n",
    "        ncds_data_no_indicators_by_group = ncds_data_no_indicators[ncds_data_no_indicators[column] == i]\n",
    "        if ncds_data_no_indicators_by_group.shape[0] > 50:\n",
    "            x = ncds_data_no_indicators_by_group.values[:, :-1]\n",
    "            y = ncds_data_no_indicators_by_group.values[:, -1]\n",
    "            x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.4, random_state=42)\n",
    "\n",
    "            features = fs.SelectKBest(fs.f_regression, k=num_of_features) #k is number of features.\n",
    "            features.fit(x_train, y_train)\n",
    "\n",
    "            selected_features = features.get_support()\n",
    "            print \"Selected Features for [\"+ str(i) +\"]:\"\n",
    "            print ncds_data_no_indicators_by_group.columns[selected_features].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.0, 1.0, -1.0]\n",
      "Selected Features for [2.0]:\n",
      "['n1region' 'n514' 'n383' 'n1819' 'n1827' 'n1828' 'n1400' 'n1399' 'n825'\n",
      " 'n1896' 'n1898' 'dvht07' 'dvrwt07' 'dvwt07' 'OUTCME01']\n",
      "Selected Features for [1.0]:\n",
      "['n400' 'n403' 'n1827' 'n604' 'n39' 'n92' 'n1400' 'n1399' 'n1476' 'n1548'\n",
      " 'n1551' 'n825' 'n1896' 'OUTCME01' 'OUTCME02']\n"
     ]
    }
   ],
   "source": [
    "### Find factors effecting seizures across different sex\n",
    "list_of_sex = list(ncds_data_no_indicators['n622'].unique())\n",
    "print list_of_sex\n",
    "\n",
    "find_features_by_group('n622',list_of_sex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 2.0, 9.0, 3.0, 4.0]\n",
      "Selected Features for [1.0]:\n",
      "['n514' 'n1827' 'n604' 'n1400' 'n1476' 'n825' 'n857' 'n2598' 'n1896'\n",
      " 'n1897' 'n1898' 'dvht07' 'OUTCME01' 'OUTCME02' 'OUTCME03']\n",
      "Selected Features for [2.0]:\n",
      "['n1819' 'n604' 'n1400' 'n1239' 'n1543' 'n1537' 'n1551' 'n825' 'n2572'\n",
      " 'n2573' 'n1896' 'n1898' 'n1964' 'n1966' 'n2154']\n",
      "Selected Features for [9.0]:\n",
      "['n95' 'n183' 'n192' 'n559' 'n459' 'n1400' 'n1221' 'n1560' 'n2406' 'n2630'\n",
      " 'n1907' 'n2241' 'n2242' 'n2289' 'n2894']\n",
      "Selected Features for [3.0]:\n",
      "['n1827' 'n481' 'n1263' 'n1400' 'n1548' 'n1551' 'n1591' 'n1635' 'n2559'\n",
      " 'n2564' 'n2596' 'n2598' 'n1940' 'n1966' 'n15']\n",
      "Selected Features for [4.0]:\n",
      "['n481' 'n482' 'n1301' 'n1364' 'n1449' 'n2539' 'n2599' 'n2610' 'n2611'\n",
      " 'n1964' 'n2158' 'n2185' 'n2252' 'n2327' 'n2331']\n"
     ]
    }
   ],
   "source": [
    "### Find factors effecting seizures based on country of birth\n",
    "list_of_cob = list(ncds_data_no_indicators['COBIRTH'].unique())\n",
    "print list_of_cob\n",
    "\n",
    "find_features_by_group('COBIRTH',list_of_cob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 5.0, 6.0, 3.0, 2.0, 4.0]\n",
      "Selected Features for [1.0]:\n",
      "['n514' 'n1827' 'n604' 'n1400' 'n1476' 'n825' 'n2598' 'n1896' 'n1897'\n",
      " 'n1898' 'n2102' 'dvht07' 'OUTCME01' 'OUTCME02' 'OUTCME03']\n",
      "Selected Features for [5.0]:\n",
      "['n500' 'n506' 'n179' 'n346' 'n558' 'n27' 'n454' 'n463' 'n472' 'n1266'\n",
      " 'n1548' 'DIASTOL' 'PRESENT' 'DTB6' 'ILLNESS']\n",
      "Selected Features for [6.0]:\n",
      "['n545' 'n506' 'n532' 'n1338' 'n1406' 'n1447' 'n1454' 'n887' 'n2618'\n",
      " 'n2621' 'n2622' 'n1935' 'n2239' 'n2241' 'n2242']\n",
      "Selected Features for [3.0]:\n",
      "['n542' 'n114' 'n127' 'n131' 'n147' 'n368' 'n380' 'n404' 'n405' 'n409'\n",
      " 'n1290' 'n1414' 'n855' 'n2294' 'n2295']\n",
      "Selected Features for [2.0]:\n",
      "['n545' 'n492' 'n494' 'n236' 'n1844' 'n1866' 'n1128' 'n1167' 'n1681'\n",
      " 'n1172' 'n857' 'n2380' 'n2437' 'n2592' 'n2257']\n"
     ]
    }
   ],
   "source": [
    "### Find factors effecting seizures based on ethinic group\n",
    "list_of_eg = list(ncds_data_no_indicators['ETHNICID'].unique())\n",
    "print list_of_eg\n",
    "\n",
    "find_features_by_group('ETHNICID',list_of_eg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Performance Metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our performance metric will be to build a better model than the results from the base models we have used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>knn</th>\n",
       "      <th>lda</th>\n",
       "      <th>linear</th>\n",
       "      <th>qda</th>\n",
       "      <th>rf</th>\n",
       "      <th>svc</th>\n",
       "      <th>tree</th>\n",
       "      <th>unweighted logistic</th>\n",
       "      <th>weighted logistic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>overall accuracy</th>\n",
       "      <td>0.909752</td>\n",
       "      <td>0.912177</td>\n",
       "      <td>0.050431</td>\n",
       "      <td>0.851832</td>\n",
       "      <td>0.799300</td>\n",
       "      <td>0.715383</td>\n",
       "      <td>0.895609</td>\n",
       "      <td>0.915948</td>\n",
       "      <td>0.623788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy on class 0</th>\n",
       "      <td>0.988218</td>\n",
       "      <td>0.988954</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.911487</td>\n",
       "      <td>0.856701</td>\n",
       "      <td>0.737113</td>\n",
       "      <td>0.964654</td>\n",
       "      <td>0.997791</td>\n",
       "      <td>0.625626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy on class 1</th>\n",
       "      <td>0.069401</td>\n",
       "      <td>0.089905</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.212934</td>\n",
       "      <td>0.184543</td>\n",
       "      <td>0.482650</td>\n",
       "      <td>0.156151</td>\n",
       "      <td>0.039432</td>\n",
       "      <td>0.604101</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          knn       lda    linear       qda        rf  \\\n",
       "overall accuracy     0.909752  0.912177  0.050431  0.851832  0.799300   \n",
       "accuracy on class 0  0.988218  0.988954  0.000000  0.911487  0.856701   \n",
       "accuracy on class 1  0.069401  0.089905  0.000000  0.212934  0.184543   \n",
       "\n",
       "                          svc      tree  unweighted logistic  \\\n",
       "overall accuracy     0.715383  0.895609             0.915948   \n",
       "accuracy on class 0  0.737113  0.964654             0.997791   \n",
       "accuracy on class 1  0.482650  0.156151             0.039432   \n",
       "\n",
       "                     weighted logistic  \n",
       "overall accuracy              0.623788  \n",
       "accuracy on class 0           0.625626  \n",
       "accuracy on class 1           0.604101  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Milestone 5 - Proposal\n",
    "\n",
    "Propose methodologies and ideas to be implemented, tested and interpreted for your final project.  Provide a roughly 1 page\n",
    "outline of what approaches, models, etc. you would like to use for the \f",
    "final project results.\n",
    "\n",
    "For the final project, we need to work on the following.\n",
    "\n",
    "#### Data Management\n",
    "\n",
    "We will supplement the process where we used means and medians to compute missing values.  K-Nearest Neighbor will be used to estimate values for blanks,'none' and 'NA'.  As KNN is compute intensive it will be used for our predictive features only.\n",
    "\n",
    "We will also separate some of the categorical values which have been encoded as numerical.  This will allow us to better evaluate their effect on our models.  In addition, by recoding them, we can use linear discriminant analysis of the textual data to better categorize factors affecting seizure occurrence into certain groups and with demographics.\n",
    "\n",
    "#### Feature Selection\n",
    "\n",
    "Cross-validation will be used to get a more accurate feature set.  The methods we used were valid, but may be biased as we only used a single sample to compute them. Further comparison of ensemble selectors will help us pinpoint the best cluster.\n",
    "\n",
    "Sweeps will be separated from each other so we can better evaluate correlation between predictors.  It appears that some of our features aren't clearly predictors.  In the case of hospital stays, seizures may be the cause instead of the effect.\n",
    "\n",
    "#### Models\n",
    "\n",
    "We can optimize all our models with enhanced cross-validation. Our first pass will leverage KFold for train and test splits.  Another approach we have discussed is to take the per-sweep data and build separate models, then combine them to have a model which can be extended to non-adolescent subjects.\n",
    "\n",
    "Our linear prediction model can be improved by further distillation of our features as discussed above.  Further gains should be had through better imputation of missing values using KNN.  Our model using logistic regression for categorization can be improved by the recoding above, and should also benefit from feature scrubbing.\n",
    "\n",
    "#### Analysis\n",
    "\n",
    "Here we can look back at our dataset and see how the models are performing. We can validate if the model prediction is actually doing a good job or not. Here we will see how we can handle false possitves. Our model tuning step will focus on improving the accuracy on both class 0 and class 1 predictions and thus reducing false posstives.\n",
    "\n",
    "#### Visualization and presentation\n",
    "\n",
    "Here we plan to tell a story of all our findings from the first step to the last. We plan to build an interactive web site to tell the story. This would be the final part of the project that ties all our work together.\n",
    "\n",
    "#### Sources\n",
    "\n",
    "Thall, P. F. and Vail, S. C. (1990) Some covariance models for longitudinal count data with over-dispersion. Biometrics 46, 657–671.\n",
    "\n",
    "University of London. Institute of Education. Centre for Longitudinal Studies. (2014). National Child Development Study: Childhood Data, Sweeps 0-3, 1958-1974. [data collection]. 3rd Edition. National Birthday Trust Fund, National Children's Bureau, [original data producer(s)]. UK Data Service. SN: 5565, http://dx.doi.org/10.5255/UKDA-SN-5565-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [py27]",
   "language": "python",
   "name": "Python [py27]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
